{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u041e \u043f\u0440\u043e\u0435\u043a\u0442\u0435 dontreadme \ud83d\ude0e API dontreadme - \u044d\u0442\u043e \u0441\u0435\u0440\u0432\u0438\u0441 \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f \u043a\u043d\u0438\u0433 \u043e\u043d\u043b\u0430\u0439\u043d. \u041c\u044b \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0434\u043e\u0441\u0442\u0443\u043f \u043a \u0431\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u043e\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 \ud83d\udcda, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0448\u0438\u0440\u043e\u043a\u0438\u0439 \u0432\u044b\u0431\u043e\u0440 \u043a\u043d\u0438\u0433 \u0440\u0430\u0437\u043d\u044b\u0445 \u0436\u0430\u043d\u0440\u043e\u0432 \u0438 \u0430\u0432\u0442\u043e\u0440\u043e\u0432. \u041a\u0430\u043a \u044d\u0442\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \ud83e\udd14 \u0427\u0442\u043e\u0431\u044b \u043d\u0430\u0447\u0430\u0442\u044c \u0447\u0438\u0442\u0430\u0442\u044c \u043a\u043d\u0438\u0433\u0443, \u043f\u0440\u043e\u0441\u0442\u043e \u043d\u0430\u0439\u0434\u0438\u0442\u0435 \u0435\u0435 \u0432 \u043d\u0430\u0448\u0435\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 \u0438 \u043e\u0442\u043a\u0440\u043e\u0439\u0442\u0435 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0443 \u043a\u043d\u0438\u0433\u0438. \u0412\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u043e \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0438\u043b\u0438 \u0432\u0432\u043e\u0434\u0438\u0442\u044c \u043a\u0430\u043a\u0443\u044e-\u043b\u0438\u0431\u043e \u043b\u0438\u0447\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e. \u041c\u044b \u0442\u0430\u043a\u0436\u0435 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0437\u0430\u043c\u0435\u0442\u043a\u0438 \ud83d\udcdd \u0438 \u0437\u0430\u043a\u043b\u0430\u0434\u043a\u0438 \ud83d\udccc, \u0447\u0442\u043e\u0431\u044b \u0432\u044b \u043c\u043e\u0433\u043b\u0438 \u0432\u0435\u0440\u043d\u0443\u0442\u044c\u0441\u044f \u043a \u043b\u044e\u0431\u043e\u043c\u0443 \u043c\u0435\u0441\u0442\u0443 \u0432 \u043a\u043d\u0438\u0433\u0435, \u043a\u043e\u0433\u0434\u0430 \u0437\u0430\u0445\u043e\u0442\u0438\u0442\u0435. \u041a\u0430\u043a \u0432\u043d\u0435\u0441\u0442\u0438 \u0441\u0432\u043e\u0439 \u0432\u043a\u043b\u0430\u0434 \ud83d\udcaa \u0415\u0441\u043b\u0438 \u0432\u044b \u0437\u0430\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043d\u044b \u0432 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0438 \u043d\u0430\u0448\u0435\u0433\u043e \u0441\u0435\u0440\u0432\u0438\u0441\u0430, \u0442\u043e \u043c\u044b \u0432\u0441\u0435\u0433\u0434\u0430 \u0440\u0430\u0434\u044b \u043d\u043e\u0432\u044b\u043c \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a\u0430\u043c. \u041d\u0430\u0448 \u043a\u043e\u0434 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u043e\u0442\u043a\u0440\u044b\u0442\u043e\u043c \u0434\u043e\u0441\u0442\u0443\u043f\u0435 \u043d\u0430 GitHub, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u043b\u0435\u0433\u043a\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0444\u043e\u0440\u043a \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u044f \u0438 \u043d\u0430\u0447\u0430\u0442\u044c \u0440\u0430\u0431\u043e\u0442\u0443. \u0421\u0432\u044f\u0437\u0430\u0442\u044c\u0441\u044f \u0441 \u043d\u0430\u043c\u0438 \ud83d\udce7 \u0415\u0441\u043b\u0438 \u0443 \u0432\u0430\u0441 \u0435\u0441\u0442\u044c \u043a\u0430\u043a\u0438\u0435-\u043b\u0438\u0431\u043e \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0438\u043b\u0438 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u043f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u0441\u0432\u044f\u0436\u0438\u0442\u0435\u0441\u044c \u0441 \u043d\u0430\u043c\u0438 \u043f\u043e \u0430\u0434\u0440\u0435\u0441\u0443 support@dontreadme.com. \u041c\u044b \u0432\u0441\u0435\u0433\u0434\u0430 \u0440\u0430\u0434\u044b \u0443\u0441\u043b\u044b\u0448\u0430\u0442\u044c \u043e\u0442 \u043d\u0430\u0448\u0438\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439! \ud83d\ude0a","title":"Home"},{"location":"#dontreadme","text":"API dontreadme - \u044d\u0442\u043e \u0441\u0435\u0440\u0432\u0438\u0441 \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f \u043a\u043d\u0438\u0433 \u043e\u043d\u043b\u0430\u0439\u043d. \u041c\u044b \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0434\u043e\u0441\u0442\u0443\u043f \u043a \u0431\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u043e\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 \ud83d\udcda, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0448\u0438\u0440\u043e\u043a\u0438\u0439 \u0432\u044b\u0431\u043e\u0440 \u043a\u043d\u0438\u0433 \u0440\u0430\u0437\u043d\u044b\u0445 \u0436\u0430\u043d\u0440\u043e\u0432 \u0438 \u0430\u0432\u0442\u043e\u0440\u043e\u0432.","title":"\u041e \u043f\u0440\u043e\u0435\u043a\u0442\u0435 dontreadme \ud83d\ude0e"},{"location":"#_1","text":"\u0427\u0442\u043e\u0431\u044b \u043d\u0430\u0447\u0430\u0442\u044c \u0447\u0438\u0442\u0430\u0442\u044c \u043a\u043d\u0438\u0433\u0443, \u043f\u0440\u043e\u0441\u0442\u043e \u043d\u0430\u0439\u0434\u0438\u0442\u0435 \u0435\u0435 \u0432 \u043d\u0430\u0448\u0435\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 \u0438 \u043e\u0442\u043a\u0440\u043e\u0439\u0442\u0435 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0443 \u043a\u043d\u0438\u0433\u0438. \u0412\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u043e \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0438\u043b\u0438 \u0432\u0432\u043e\u0434\u0438\u0442\u044c \u043a\u0430\u043a\u0443\u044e-\u043b\u0438\u0431\u043e \u043b\u0438\u0447\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e. \u041c\u044b \u0442\u0430\u043a\u0436\u0435 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0437\u0430\u043c\u0435\u0442\u043a\u0438 \ud83d\udcdd \u0438 \u0437\u0430\u043a\u043b\u0430\u0434\u043a\u0438 \ud83d\udccc, \u0447\u0442\u043e\u0431\u044b \u0432\u044b \u043c\u043e\u0433\u043b\u0438 \u0432\u0435\u0440\u043d\u0443\u0442\u044c\u0441\u044f \u043a \u043b\u044e\u0431\u043e\u043c\u0443 \u043c\u0435\u0441\u0442\u0443 \u0432 \u043a\u043d\u0438\u0433\u0435, \u043a\u043e\u0433\u0434\u0430 \u0437\u0430\u0445\u043e\u0442\u0438\u0442\u0435.","title":"\u041a\u0430\u043a \u044d\u0442\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \ud83e\udd14"},{"location":"#_2","text":"\u0415\u0441\u043b\u0438 \u0432\u044b \u0437\u0430\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043d\u044b \u0432 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0438 \u043d\u0430\u0448\u0435\u0433\u043e \u0441\u0435\u0440\u0432\u0438\u0441\u0430, \u0442\u043e \u043c\u044b \u0432\u0441\u0435\u0433\u0434\u0430 \u0440\u0430\u0434\u044b \u043d\u043e\u0432\u044b\u043c \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a\u0430\u043c. \u041d\u0430\u0448 \u043a\u043e\u0434 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u043e\u0442\u043a\u0440\u044b\u0442\u043e\u043c \u0434\u043e\u0441\u0442\u0443\u043f\u0435 \u043d\u0430 GitHub, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u043b\u0435\u0433\u043a\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0444\u043e\u0440\u043a \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u044f \u0438 \u043d\u0430\u0447\u0430\u0442\u044c \u0440\u0430\u0431\u043e\u0442\u0443.","title":"\u041a\u0430\u043a \u0432\u043d\u0435\u0441\u0442\u0438 \u0441\u0432\u043e\u0439 \u0432\u043a\u043b\u0430\u0434 \ud83d\udcaa"},{"location":"#_3","text":"\u0415\u0441\u043b\u0438 \u0443 \u0432\u0430\u0441 \u0435\u0441\u0442\u044c \u043a\u0430\u043a\u0438\u0435-\u043b\u0438\u0431\u043e \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0438\u043b\u0438 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u043f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u0441\u0432\u044f\u0436\u0438\u0442\u0435\u0441\u044c \u0441 \u043d\u0430\u043c\u0438 \u043f\u043e \u0430\u0434\u0440\u0435\u0441\u0443 support@dontreadme.com. \u041c\u044b \u0432\u0441\u0435\u0433\u0434\u0430 \u0440\u0430\u0434\u044b \u0443\u0441\u043b\u044b\u0448\u0430\u0442\u044c \u043e\u0442 \u043d\u0430\u0448\u0438\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439! \ud83d\ude0a","title":"\u0421\u0432\u044f\u0437\u0430\u0442\u044c\u0441\u044f \u0441 \u043d\u0430\u043c\u0438 \ud83d\udce7"},{"location":"api/","text":"API About Recomendation recomend.py popular_items ( data , data_items , genre = None , threshold_progress = 40 , n_items = 10 ) Recomends the top n popular items for a given genre. Parameters pd.DataFrame dataframe containing the user-item interactions pd.DataFrame dataframe containing the items str genre of the items to be recommended int, optional threshold of progress items, by default 40 int, optional count items to be return, by default 10 Returns np.ndarray the top n popular items for a given genre Source code in src/recomend.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def popular_items ( data : pd . DataFrame , data_items : pd . DataFrame , genre : str | None = None , threshold_progress : int = 40 , n_items : int = 10 , ): \"\"\"Recomends the top n popular items for a given genre. Parameters ---------- df : pd.DataFrame dataframe containing the user-item interactions df_items : pd.DataFrame dataframe containing the items genre : str genre of the items to be recommended threshold_progress : int, optional threshold of progress items, by default 40 n : int, optional count items to be return, by default 10 Returns ------- output: np.ndarray the top n popular items for a given genre \"\"\" mask = data [ data [ \"progress\" ] > threshold_progress ][[ \"item_id\" ]] mask = mask . value_counts () items_count = pd . DataFrame ( mask , columns = [ \"count\" ], ) . sort_index () items_name = data_items [[ \"id\" , \"title\" , \"genres\" ]] items_name = items_name . set_index ( \"id\" ) items_name = items_name . sort_index () items_name . genres = items_name . genres . fillna ( \"\u0414\u0440\u0443\u0433\u0438\u0435 \u0436\u0430\u043d\u0440\u044b\" ) items_name . genres = items_name . genres . apply ( lambda x : x . split ( \",\" )) if genre is not None : items_name = items_name . explode ( column = \"genres\" ) items_name = items_name [ items_name [ \"genres\" ] == genre ][[ \"title\" ]] count_titles = items_name . merge ( items_count , left_index = True , right_on = \"item_id\" , ) output = count_titles . sort_values ( by = \"count\" , ascending = False ) output = output [ \"title\" ] . values [: n_items ] return output Utils Utils. get_coo_matrix ( intercations , user_col = 'user_id' , item_col = 'item_id' , weight_col = None , users_mapping = None , items_mapping = None ) Create a COO sparse matrix from a pandas DataFrame. Parameters pd.DataFrame DataFrame containing the user-item interactions. str, optional Name of the user ID column in df , by default 'user_id'. str, optional Name of the item ID column in df , by default 'item_id'. str, optional Name of the weight column in df , by default None. dict, optional A mapping from user IDs to row indices in the resulting matrix, by default None. dict, optional A mapping from item IDs to column indices in the resulting matrix, by default None. Returns sp.coo_matrix A sparse COO matrix representing the user-item interactions. Source code in src/utils.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_coo_matrix ( intercations : pd . DataFrame , user_col : str = \"user_id\" , item_col : str = \"item_id\" , weight_col : str = None , users_mapping : dict = None , items_mapping : dict = None , ) -> sp . coo_matrix : \"\"\"Create a COO sparse matrix from a pandas DataFrame. Parameters ---------- df : pd.DataFrame DataFrame containing the user-item interactions. user_col : str, optional Name of the user ID column in `df`, by default 'user_id'. item_col : str, optional Name of the item ID column in `df`, by default 'item_id'. weight_col : str, optional Name of the weight column in `df`, by default None. users_mapping : dict, optional A mapping from user IDs to row indices in the resulting matrix, by default None. items_mapping : dict, optional A mapping from item IDs to column indices in the resulting matrix, by default None. Returns ------- sp.coo_matrix A sparse COO matrix representing the user-item interactions. \"\"\" if weight_col is None : weights = np . ones ( len ( intercations ), dtype = np . float32 ) else : weights = intercations [ weight_col ] . astype ( np . float32 ) interaction_matrix = sp . coo_matrix ( ( weights , ( intercations [ user_col ] . map ( users_mapping . get ), intercations [ item_col ] . map ( items_mapping . get ), ), ), ) return interaction_matrix get_mapping ( intercations ) Returns two mappings for user and item IDs to integer indices. Parameters pandas.DataFrame The input DataFrame containing user and item IDs. Returns tuple of two dictionaries The first dictionary maps user IDs to integer indices, and the second dictionary maps item IDs to integer indices. Source code in src/utils.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def get_mapping ( intercations : pd . DataFrame ): \"\"\" Returns two mappings for user and item IDs to integer indices. Parameters ---------- df : pandas.DataFrame The input DataFrame containing user and item IDs. Returns ------- tuple of two dictionaries The first dictionary maps user IDs to integer indices, and the second dictionary maps item IDs to integer indices. \"\"\" users_inv_mapping = dict ( enumerate ( intercations [ \"user_id\" ] . unique ())) users_mapping = { v : k for k , v in users_inv_mapping . items ()} items_inv_mapping = dict ( enumerate ( intercations [ \"item_id\" ] . unique ())) items_mapping = { v : k for k , v in items_inv_mapping . items ()} return users_mapping , items_mapping get_user_loggins ( data ) Extracts unique user IDs from the given data. Source code in src/utils.py 123 124 125 126 def get_user_loggins ( data ): \"\"\"Extracts unique user IDs from the given data.\"\"\" users = data [ 'user_id' ] . unique () return list ( users ) read_data ( path ) Reads data from csv files located at given path. Parameters str The path where the csv files are located. Returns tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame] A tuple containing three pandas DataFrames representing the data, users, and items csv files, respectively. Raises AssertionError If any of the csv files are not found at the given path. Source code in src/utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def read_data ( path : str ) -> tuple [ pd . DataFrame , pd . DataFrame , pd . DataFrame ]: \"\"\" Reads data from csv files located at given path. Parameters ---------- path : str The path where the csv files are located. Returns ------- tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame] A tuple containing three pandas DataFrames representing the data, users, and items csv files, respectively. Raises ------ AssertionError If any of the csv files are not found at the given path. \"\"\" path_data = os . path . join ( path , \"data/interactions.csv\" ) path_items = os . path . join ( path , \"data/items.csv\" ) path_users = os . path . join ( path , \"data/users.csv\" ) # logger.info(f\"Reading data from csv files {path_data}\") assert os . path . exists ( path_data ), f \"File { path_data } not found.\" assert os . path . exists ( path_items ), f \"File { path_data } not found.\" assert os . path . exists ( path_users ), f \"File { path_data } not found.\" data = pd . read_csv ( path_data ) data_users = pd . read_csv ( path_users ) data_items = pd . read_csv ( path_items ) data [ \"start_date\" ] = pd . to_datetime ( data [ \"start_date\" ]) data [ \"rating\" ] = np . array ( data [ \"rating\" ] . values , dtype = np . float32 ) return data , data_users , data_items Validation TimeRangeSplit From https://www.kaggle.com/code/sharthz23/implicit-lightfm. Source code in src/validation.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class TimeRangeSplit : \"\"\"From https://www.kaggle.com/code/sharthz23/implicit-lightfm.\"\"\" def __init__ ( self , start_date , end_date = None , freq = \"D\" , periods = None , tz = None , normalize = False , closed = None , train_min_date = None , filter_cold_users = True , filter_cold_items = True , filter_already_seen = True , ): \"\"\"Initialize TimeRangeSplit.\"\"\" self . start_date = start_date if end_date is None and periods is None : raise ValueError ( \"Either 'end_date' or 'periods' must be non-zero, not both at the same time.\" , ) self . end_date = end_date self . freq = freq self . periods = periods self . tz = tz self . normalize = normalize self . closed = closed self . train_min_date = pd . to_datetime ( train_min_date , errors = \"raise\" ) self . filter_cold_users = filter_cold_users self . filter_cold_items = filter_cold_items self . filter_already_seen = filter_already_seen self . date_range = pd . date_range ( start = start_date , end = end_date , freq = freq , periods = periods , tz = tz , normalize = normalize , closed = closed , ) self . max_n_splits = max ( 0 , len ( self . date_range ) - 1 ) if self . max_n_splits == 0 : raise ValueError ( \"Provided parametrs set an empty date range.\" ) def split ( self , df , user_column = \"user_id\" , item_column = \"item_id\" , datetime_column = \"date\" , fold_stats = False , ): \"\"\"Split the dataset into training and test sets.\"\"\" df_datetime = df [ datetime_column ] if self . train_min_date is not None : train_min_mask = df_datetime >= self . train_min_date else : train_min_mask = df_datetime . notnull () date_range = self . date_range [ ( self . date_range >= df_datetime . min ()) & ( self . date_range <= df_datetime . max ()) ] for start , end in pairwise ( date_range ): fold_info = { \"Start date\" : start , \"End date\" : end } train_mask = train_min_mask & ( df_datetime < start ) train_idx = df . index [ train_mask ] if fold_stats : fold_info [ \"Train\" ] = len ( train_idx ) test_mask = ( df_datetime >= start ) & ( df_datetime < end ) test_idx = df . index [ test_mask ] if self . filter_cold_users : new = np . setdiff1d ( df . loc [ test_idx , user_column ] . unique (), df . loc [ train_idx , user_column ] . unique (), ) new_idx = df . index [ test_mask & df [ user_column ] . isin ( new )] test_idx = np . setdiff1d ( test_idx , new_idx ) test_mask = df . index . isin ( test_idx ) if fold_stats : fold_info [ \"New users\" ] = len ( new ) fold_info [ \"New users interactions\" ] = len ( new_idx ) if self . filter_cold_items : new = np . setdiff1d ( df . loc [ test_idx , item_column ] . unique (), df . loc [ train_idx , item_column ] . unique (), ) new_idx = df . index [ test_mask & df [ item_column ] . isin ( new )] test_idx = np . setdiff1d ( test_idx , new_idx ) test_mask = df . index . isin ( test_idx ) if fold_stats : fold_info [ \"New items\" ] = len ( new ) fold_info [ \"New items interactions\" ] = len ( new_idx ) if self . filter_already_seen : user_item = [ user_column , item_column ] train_pairs = df . loc [ train_idx , user_item ] . set_index ( user_item ) . index test_pairs = df . loc [ test_idx , user_item ] . set_index ( user_item ) . index intersection = train_pairs . intersection ( test_pairs ) test_idx = test_idx [ ~ test_pairs . isin ( intersection )] # test_mask = rd.df.index.isin(test_idx) if fold_stats : fold_info [ \"Known interactions\" ] = len ( intersection ) if fold_stats : fold_info [ \"Test\" ] = len ( test_idx ) yield ( train_idx , test_idx , fold_info ) def get_n_splits ( self , df , datetime_column = \"date\" ): \"\"\"Get n splits.\"\"\" df_datetime = df [ datetime_column ] if self . train_min_date is not None : df_datetime = df_datetime [ df_datetime >= self . train_min_date ] date_range = self . date_range [ ( self . date_range >= df_datetime . min ()) & ( self . date_range <= df_datetime . max ()) ] return max ( 0 , len ( date_range ) - 1 ) __init__ ( start_date , end_date = None , freq = 'D' , periods = None , tz = None , normalize = False , closed = None , train_min_date = None , filter_cold_users = True , filter_cold_items = True , filter_already_seen = True ) Initialize TimeRangeSplit. Source code in src/validation.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , start_date , end_date = None , freq = \"D\" , periods = None , tz = None , normalize = False , closed = None , train_min_date = None , filter_cold_users = True , filter_cold_items = True , filter_already_seen = True , ): \"\"\"Initialize TimeRangeSplit.\"\"\" self . start_date = start_date if end_date is None and periods is None : raise ValueError ( \"Either 'end_date' or 'periods' must be non-zero, not both at the same time.\" , ) self . end_date = end_date self . freq = freq self . periods = periods self . tz = tz self . normalize = normalize self . closed = closed self . train_min_date = pd . to_datetime ( train_min_date , errors = \"raise\" ) self . filter_cold_users = filter_cold_users self . filter_cold_items = filter_cold_items self . filter_already_seen = filter_already_seen self . date_range = pd . date_range ( start = start_date , end = end_date , freq = freq , periods = periods , tz = tz , normalize = normalize , closed = closed , ) self . max_n_splits = max ( 0 , len ( self . date_range ) - 1 ) if self . max_n_splits == 0 : raise ValueError ( \"Provided parametrs set an empty date range.\" ) get_n_splits ( df , datetime_column = 'date' ) Get n splits. Source code in src/validation.py 137 138 139 140 141 142 143 144 145 146 147 148 def get_n_splits ( self , df , datetime_column = \"date\" ): \"\"\"Get n splits.\"\"\" df_datetime = df [ datetime_column ] if self . train_min_date is not None : df_datetime = df_datetime [ df_datetime >= self . train_min_date ] date_range = self . date_range [ ( self . date_range >= df_datetime . min ()) & ( self . date_range <= df_datetime . max ()) ] return max ( 0 , len ( date_range ) - 1 ) split ( df , user_column = 'user_id' , item_column = 'item_id' , datetime_column = 'date' , fold_stats = False ) Split the dataset into training and test sets. Source code in src/validation.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def split ( self , df , user_column = \"user_id\" , item_column = \"item_id\" , datetime_column = \"date\" , fold_stats = False , ): \"\"\"Split the dataset into training and test sets.\"\"\" df_datetime = df [ datetime_column ] if self . train_min_date is not None : train_min_mask = df_datetime >= self . train_min_date else : train_min_mask = df_datetime . notnull () date_range = self . date_range [ ( self . date_range >= df_datetime . min ()) & ( self . date_range <= df_datetime . max ()) ] for start , end in pairwise ( date_range ): fold_info = { \"Start date\" : start , \"End date\" : end } train_mask = train_min_mask & ( df_datetime < start ) train_idx = df . index [ train_mask ] if fold_stats : fold_info [ \"Train\" ] = len ( train_idx ) test_mask = ( df_datetime >= start ) & ( df_datetime < end ) test_idx = df . index [ test_mask ] if self . filter_cold_users : new = np . setdiff1d ( df . loc [ test_idx , user_column ] . unique (), df . loc [ train_idx , user_column ] . unique (), ) new_idx = df . index [ test_mask & df [ user_column ] . isin ( new )] test_idx = np . setdiff1d ( test_idx , new_idx ) test_mask = df . index . isin ( test_idx ) if fold_stats : fold_info [ \"New users\" ] = len ( new ) fold_info [ \"New users interactions\" ] = len ( new_idx ) if self . filter_cold_items : new = np . setdiff1d ( df . loc [ test_idx , item_column ] . unique (), df . loc [ train_idx , item_column ] . unique (), ) new_idx = df . index [ test_mask & df [ item_column ] . isin ( new )] test_idx = np . setdiff1d ( test_idx , new_idx ) test_mask = df . index . isin ( test_idx ) if fold_stats : fold_info [ \"New items\" ] = len ( new ) fold_info [ \"New items interactions\" ] = len ( new_idx ) if self . filter_already_seen : user_item = [ user_column , item_column ] train_pairs = df . loc [ train_idx , user_item ] . set_index ( user_item ) . index test_pairs = df . loc [ test_idx , user_item ] . set_index ( user_item ) . index intersection = train_pairs . intersection ( test_pairs ) test_idx = test_idx [ ~ test_pairs . isin ( intersection )] # test_mask = rd.df.index.isin(test_idx) if fold_stats : fold_info [ \"Known interactions\" ] = len ( intersection ) if fold_stats : fold_info [ \"Test\" ] = len ( test_idx ) yield ( train_idx , test_idx , fold_info ) validation_als ( folds_with_stats , df , users_mapping , items_mapping ) Perform k-fold cross-validation on the ALS model. Parameters list A list of (train_idx, test_idx, stats) tuples, where train_idx and test_idx are the indices of the training and test sets, respectively, and stats is a dictionary containing statistics about the fold. pandas.DataFrame The interaction matrix, with columns \"user_id\", \"item_id\", and \"weight\". Returns None Source code in src/validation.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def validation_als ( folds_with_stats , df , users_mapping , items_mapping ): \"\"\" Perform k-fold cross-validation on the ALS model. Parameters ---------- folds_with_stats : list A list of (train_idx, test_idx, stats) tuples, where train_idx and test_idx are the indices of the training and test sets, respectively, and stats is a dictionary containing statistics about the fold. df : pandas.DataFrame The interaction matrix, with columns \"user_id\", \"item_id\", and \"weight\". Returns ------- None \"\"\" wandb . init ( project = \"MFDP\" , name = \"ALS\" ) run_no = 0 for train_idx , test_idx , _ in folds_with_stats : run_no += 1 train = df . loc [ train_idx ] test = df . loc [ test_idx ] train_mat = get_coo_matrix ( train , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () test_mat = get_coo_matrix ( test , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = AlternatingLeastSquares ( factors = 32 , iterations = 30 ) model . fit ( train_mat . T , show_progress = False ) metrics = ranking_metrics_at_k ( model , train_mat . T , test_mat . T , K = 10 , show_progress = False , ) metrics [ \"fold\" ] = run_no wandb . log ( metrics ) data_mat = get_coo_matrix ( df , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = AlternatingLeastSquares ( factors = 32 , iterations = 30 ) model . fit ( data_mat . T , show_progress = False ) with open ( os . path . join ( sys . path [ 0 ], \"./model/als.pickle\" ), \"wb\" ) as f : pickle . dump ( model , f ) artifact = wandb . Artifact ( \"model\" , type = \"model\" ) with artifact . new_file ( os . path . join ( sys . path [ 0 ], \"./model/als.pickle\" ), mode = \"wb\" ) as file : pickle . dump ( model , file ) wandb . log_artifact ( artifact ) wandb . finish () validation_bm25 ( folds_with_stats , df , users_mapping , items_mapping ) Validate TF-IDF recommender using k-fold cross-validation and log results to WandB. Parameters list List of tuples containing train and test indices and fold statistics. pd.DataFrame Input dataframe containing user-item interaction data. dict Dictionary mapping user IDs to integer indices. dict Dictionary mapping item IDs to integer indices. Returns None Source code in src/validation.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 def validation_bm25 ( folds_with_stats , df , users_mapping , items_mapping ): \"\"\" Validate TF-IDF recommender using k-fold cross-validation and log results to WandB. Parameters ---------- folds_with_stats : list List of tuples containing train and test indices and fold statistics. df : pd.DataFrame Input dataframe containing user-item interaction data. users_mapping : dict Dictionary mapping user IDs to integer indices. items_mapping : dict Dictionary mapping item IDs to integer indices. Returns ------- None \"\"\" wandb . init ( project = \"MFDP\" , name = \"BM25\" ) run_no = 0 for train_idx , test_idx , _ in folds_with_stats : run_no += 1 train = df . loc [ train_idx ] test = df . loc [ test_idx ] train_mat = get_coo_matrix ( train , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () test_mat = get_coo_matrix ( test , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = BM25Recommender ( K = 10 ) model . fit ( train_mat , show_progress = False ) metrics = ranking_metrics_at_k ( model , train_mat , test_mat , K = 10 , show_progress = False , ) metrics [ \"fold\" ] = run_no wandb . log ( metrics ) data_mat = get_coo_matrix ( df , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = BM25Recommender ( K = 10 ) model . fit ( data_mat , show_progress = False ) with open ( os . path . join ( sys . path [ 0 ], \"./model/bm25.pickle\" ), \"wb\" ) as f : pickle . dump ( model , f ) artifact = wandb . Artifact ( \"model\" , type = \"model\" ) with artifact . new_file ( os . path . join ( sys . path [ 0 ], \"./model/bm25.pickle\" ), mode = \"wb\" ) as file : pickle . dump ( model , file ) wandb . log_artifact ( artifact ) wandb . finish () validation_bpr ( folds_with_stats , df , users_mapping , items_mapping ) Run k-fold cross-validation for Bayesian Personalized Ranking (BPR). For each fold, fit a BPR model on the training set, and evaluate it on the test set using ranking metrics@k. The metrics are printed to the console. Parameters list of tuple of 3 arrays A list of folds, where each fold is represented as a tuple of 3 arrays: the training indices, test indices, and a dictionary of fold statistics. pandas.DataFrame The input DataFrame containing the user-item interactions. Returns None Source code in src/validation.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def validation_bpr ( folds_with_stats , df , users_mapping , items_mapping ): \"\"\"Run k-fold cross-validation for Bayesian Personalized Ranking (BPR). For each fold, fit a BPR model on the training set, and evaluate it on the test set using ranking metrics@k. The metrics are printed to the console. Parameters ---------- folds_with_stats : list of tuple of 3 arrays A list of folds, where each fold is represented as a tuple of 3 arrays: the training indices, test indices, and a dictionary of fold statistics. df : pandas.DataFrame The input DataFrame containing the user-item interactions. Returns ------- None \"\"\" wandb . init ( project = \"MFDP\" , name = \"BPR\" ) run_no = 0 for train_idx , test_idx , _ in folds_with_stats : run_no += 1 train = df . loc [ train_idx ] test = df . loc [ test_idx ] train_mat = get_coo_matrix ( train , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () test_mat = get_coo_matrix ( test , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = BayesianPersonalizedRanking ( factors = 32 , iterations = 30 ) model . fit ( train_mat . T , show_progress = False ) metrics = ranking_metrics_at_k ( model , train_mat . T , test_mat . T , K = 10 , show_progress = False , ) metrics [ \"fold\" ] = run_no wandb . log ( metrics ) data_mat = get_coo_matrix ( df , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = BayesianPersonalizedRanking ( factors = 32 , iterations = 30 ) model . fit ( data_mat . T , show_progress = False ) with open ( os . path . join ( sys . path [ 0 ], \"./model/bpr.pickle\" ), \"wb\" ) as f : pickle . dump ( model , f ) artifact = wandb . Artifact ( \"model\" , type = \"model\" ) with artifact . new_file ( os . path . join ( sys . path [ 0 ], \"./model/bpr.pickle\" ), mode = \"wb\" ) as file : pickle . dump ( model , file ) wandb . log_artifact ( artifact ) wandb . finish () validation_tfidf ( folds_with_stats , df , users_mapping , items_mapping ) Train and evaluate a TF-IDF recommender on cross-validation folds. Parameters List[Tuple[np.ndarray, np.ndarray, Dict[str, Any]]] List of tuples where each tuple contains train indices, test indices, and some stats about the fold. pd.DataFrame The ratings dataframe. Dict Mapping from user ids to integers. Dict Mapping from item ids to integers. Returns None Source code in src/validation.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def validation_tfidf ( folds_with_stats , df , users_mapping , items_mapping ): \"\"\" Train and evaluate a TF-IDF recommender on cross-validation folds. Parameters ---------- folds_with_stats : List[Tuple[np.ndarray, np.ndarray, Dict[str, Any]]] List of tuples where each tuple contains train indices, test indices, and some stats about the fold. df : pd.DataFrame The ratings dataframe. users_mapping : Dict Mapping from user ids to integers. items_mapping : Dict Mapping from item ids to integers. Returns ------- None \"\"\" wandb . init ( project = \"MFDP\" , name = \"TF-IDF\" ) run_no = 0 for train_idx , test_idx , _ in folds_with_stats : run_no += 1 train = df . loc [ train_idx ] test = df . loc [ test_idx ] train_mat = get_coo_matrix ( train , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () test_mat = get_coo_matrix ( test , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = TFIDFRecommender ( K = 10 ) model . fit ( train_mat , show_progress = False ) metrics = ranking_metrics_at_k ( model , train_mat , test_mat , K = 10 , show_progress = False , ) metrics [ \"fold\" ] = run_no wandb . log ( metrics ) data_mat = get_coo_matrix ( df , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = TFIDFRecommender ( K = 10 ) model . fit ( data_mat , show_progress = False ) with open ( os . path . join ( sys . path [ 0 ], \"./model/tfidf.pickle\" ), \"wb\" ) as f : pickle . dump ( model , f ) artifact = wandb . Artifact ( \"model\" , type = \"model\" ) with artifact . new_file ( os . path . join ( sys . path [ 0 ], \"./model/tfidf.pickle\" ), mode = \"wb\" ) as file : pickle . dump ( model , file ) wandb . log_artifact ( artifact ) wandb . finish () App create_columns_with_data ( st , k , data ) Creates k columns with data in Streamlit. Parameters streamlit The Streamlit library. int The number of columns to create. list The list of dictionaries containing the data. Source code in app.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def create_columns_with_data ( st , k : int , data : list ): \"\"\" Creates k columns with data in Streamlit. Parameters ---------- st : streamlit The Streamlit library. k : int The number of columns to create. data : list The list of dictionaries containing the data. \"\"\" if len ( data ) < k : st . error ( \"The number of data points should be greater than or equal to k.\" ) return num_data = len ( data ) num_columns = min ( k , num_data ) columns = st . columns ( num_columns ) for i in range ( num_columns ): item = data . iloc [ i ] # Search for a random photo on Unsplash # photo = api.photo.random() # Get the URL of the photo # photo_url = photo[0].urls.raw # Download the image # response = requests.get(photo_url) # img = Image.open(BytesIO(response.content)) # Resize the image to 400x300 pixels # img = img.resize((200, 300)) # Display the image # columns[i].image(img, width=200) # Display the title columns [ i ] . write ( f '** { item [ \"title\" ] } **' ) # Display the author, year, and genres columns [ i ] . write ( f \"**\u0410\u0432\u0442\u043e\u0440:** { item [ 'authors' ] } \" ) columns [ i ] . write ( f \"**\u0413\u043e\u0434:** { item [ 'year' ] } \" ) columns [ i ] . write ( f \"**\u0416\u0430\u043d\u0440\u044b:** { item [ 'genres' ] } \" )","title":"API Reference"},{"location":"api/#api","text":"About","title":"API"},{"location":"api/#recomendation","text":"recomend.py","title":"Recomendation"},{"location":"api/#src.recomend.popular_items","text":"Recomends the top n popular items for a given genre. Parameters pd.DataFrame dataframe containing the user-item interactions pd.DataFrame dataframe containing the items str genre of the items to be recommended int, optional threshold of progress items, by default 40 int, optional count items to be return, by default 10","title":"popular_items()"},{"location":"api/#src.recomend.popular_items--returns","text":"np.ndarray the top n popular items for a given genre Source code in src/recomend.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def popular_items ( data : pd . DataFrame , data_items : pd . DataFrame , genre : str | None = None , threshold_progress : int = 40 , n_items : int = 10 , ): \"\"\"Recomends the top n popular items for a given genre. Parameters ---------- df : pd.DataFrame dataframe containing the user-item interactions df_items : pd.DataFrame dataframe containing the items genre : str genre of the items to be recommended threshold_progress : int, optional threshold of progress items, by default 40 n : int, optional count items to be return, by default 10 Returns ------- output: np.ndarray the top n popular items for a given genre \"\"\" mask = data [ data [ \"progress\" ] > threshold_progress ][[ \"item_id\" ]] mask = mask . value_counts () items_count = pd . DataFrame ( mask , columns = [ \"count\" ], ) . sort_index () items_name = data_items [[ \"id\" , \"title\" , \"genres\" ]] items_name = items_name . set_index ( \"id\" ) items_name = items_name . sort_index () items_name . genres = items_name . genres . fillna ( \"\u0414\u0440\u0443\u0433\u0438\u0435 \u0436\u0430\u043d\u0440\u044b\" ) items_name . genres = items_name . genres . apply ( lambda x : x . split ( \",\" )) if genre is not None : items_name = items_name . explode ( column = \"genres\" ) items_name = items_name [ items_name [ \"genres\" ] == genre ][[ \"title\" ]] count_titles = items_name . merge ( items_count , left_index = True , right_on = \"item_id\" , ) output = count_titles . sort_values ( by = \"count\" , ascending = False ) output = output [ \"title\" ] . values [: n_items ] return output","title":"Returns"},{"location":"api/#utils","text":"Utils.","title":"Utils"},{"location":"api/#src.utils.get_coo_matrix","text":"Create a COO sparse matrix from a pandas DataFrame.","title":"get_coo_matrix()"},{"location":"api/#src.utils.get_coo_matrix--parameters","text":"pd.DataFrame DataFrame containing the user-item interactions. str, optional Name of the user ID column in df , by default 'user_id'. str, optional Name of the item ID column in df , by default 'item_id'. str, optional Name of the weight column in df , by default None. dict, optional A mapping from user IDs to row indices in the resulting matrix, by default None. dict, optional A mapping from item IDs to column indices in the resulting matrix, by default None.","title":"Parameters"},{"location":"api/#src.utils.get_coo_matrix--returns","text":"sp.coo_matrix A sparse COO matrix representing the user-item interactions. Source code in src/utils.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_coo_matrix ( intercations : pd . DataFrame , user_col : str = \"user_id\" , item_col : str = \"item_id\" , weight_col : str = None , users_mapping : dict = None , items_mapping : dict = None , ) -> sp . coo_matrix : \"\"\"Create a COO sparse matrix from a pandas DataFrame. Parameters ---------- df : pd.DataFrame DataFrame containing the user-item interactions. user_col : str, optional Name of the user ID column in `df`, by default 'user_id'. item_col : str, optional Name of the item ID column in `df`, by default 'item_id'. weight_col : str, optional Name of the weight column in `df`, by default None. users_mapping : dict, optional A mapping from user IDs to row indices in the resulting matrix, by default None. items_mapping : dict, optional A mapping from item IDs to column indices in the resulting matrix, by default None. Returns ------- sp.coo_matrix A sparse COO matrix representing the user-item interactions. \"\"\" if weight_col is None : weights = np . ones ( len ( intercations ), dtype = np . float32 ) else : weights = intercations [ weight_col ] . astype ( np . float32 ) interaction_matrix = sp . coo_matrix ( ( weights , ( intercations [ user_col ] . map ( users_mapping . get ), intercations [ item_col ] . map ( items_mapping . get ), ), ), ) return interaction_matrix","title":"Returns"},{"location":"api/#src.utils.get_mapping","text":"Returns two mappings for user and item IDs to integer indices.","title":"get_mapping()"},{"location":"api/#src.utils.get_mapping--parameters","text":"pandas.DataFrame The input DataFrame containing user and item IDs.","title":"Parameters"},{"location":"api/#src.utils.get_mapping--returns","text":"tuple of two dictionaries The first dictionary maps user IDs to integer indices, and the second dictionary maps item IDs to integer indices. Source code in src/utils.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def get_mapping ( intercations : pd . DataFrame ): \"\"\" Returns two mappings for user and item IDs to integer indices. Parameters ---------- df : pandas.DataFrame The input DataFrame containing user and item IDs. Returns ------- tuple of two dictionaries The first dictionary maps user IDs to integer indices, and the second dictionary maps item IDs to integer indices. \"\"\" users_inv_mapping = dict ( enumerate ( intercations [ \"user_id\" ] . unique ())) users_mapping = { v : k for k , v in users_inv_mapping . items ()} items_inv_mapping = dict ( enumerate ( intercations [ \"item_id\" ] . unique ())) items_mapping = { v : k for k , v in items_inv_mapping . items ()} return users_mapping , items_mapping","title":"Returns"},{"location":"api/#src.utils.get_user_loggins","text":"Extracts unique user IDs from the given data. Source code in src/utils.py 123 124 125 126 def get_user_loggins ( data ): \"\"\"Extracts unique user IDs from the given data.\"\"\" users = data [ 'user_id' ] . unique () return list ( users )","title":"get_user_loggins()"},{"location":"api/#src.utils.read_data","text":"Reads data from csv files located at given path.","title":"read_data()"},{"location":"api/#src.utils.read_data--parameters","text":"str The path where the csv files are located.","title":"Parameters"},{"location":"api/#src.utils.read_data--returns","text":"tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame] A tuple containing three pandas DataFrames representing the data, users, and items csv files, respectively.","title":"Returns"},{"location":"api/#src.utils.read_data--raises","text":"AssertionError If any of the csv files are not found at the given path. Source code in src/utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def read_data ( path : str ) -> tuple [ pd . DataFrame , pd . DataFrame , pd . DataFrame ]: \"\"\" Reads data from csv files located at given path. Parameters ---------- path : str The path where the csv files are located. Returns ------- tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame] A tuple containing three pandas DataFrames representing the data, users, and items csv files, respectively. Raises ------ AssertionError If any of the csv files are not found at the given path. \"\"\" path_data = os . path . join ( path , \"data/interactions.csv\" ) path_items = os . path . join ( path , \"data/items.csv\" ) path_users = os . path . join ( path , \"data/users.csv\" ) # logger.info(f\"Reading data from csv files {path_data}\") assert os . path . exists ( path_data ), f \"File { path_data } not found.\" assert os . path . exists ( path_items ), f \"File { path_data } not found.\" assert os . path . exists ( path_users ), f \"File { path_data } not found.\" data = pd . read_csv ( path_data ) data_users = pd . read_csv ( path_users ) data_items = pd . read_csv ( path_items ) data [ \"start_date\" ] = pd . to_datetime ( data [ \"start_date\" ]) data [ \"rating\" ] = np . array ( data [ \"rating\" ] . values , dtype = np . float32 ) return data , data_users , data_items","title":"Raises"},{"location":"api/#validation","text":"","title":"Validation"},{"location":"api/#src.validation.TimeRangeSplit","text":"From https://www.kaggle.com/code/sharthz23/implicit-lightfm. Source code in src/validation.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class TimeRangeSplit : \"\"\"From https://www.kaggle.com/code/sharthz23/implicit-lightfm.\"\"\" def __init__ ( self , start_date , end_date = None , freq = \"D\" , periods = None , tz = None , normalize = False , closed = None , train_min_date = None , filter_cold_users = True , filter_cold_items = True , filter_already_seen = True , ): \"\"\"Initialize TimeRangeSplit.\"\"\" self . start_date = start_date if end_date is None and periods is None : raise ValueError ( \"Either 'end_date' or 'periods' must be non-zero, not both at the same time.\" , ) self . end_date = end_date self . freq = freq self . periods = periods self . tz = tz self . normalize = normalize self . closed = closed self . train_min_date = pd . to_datetime ( train_min_date , errors = \"raise\" ) self . filter_cold_users = filter_cold_users self . filter_cold_items = filter_cold_items self . filter_already_seen = filter_already_seen self . date_range = pd . date_range ( start = start_date , end = end_date , freq = freq , periods = periods , tz = tz , normalize = normalize , closed = closed , ) self . max_n_splits = max ( 0 , len ( self . date_range ) - 1 ) if self . max_n_splits == 0 : raise ValueError ( \"Provided parametrs set an empty date range.\" ) def split ( self , df , user_column = \"user_id\" , item_column = \"item_id\" , datetime_column = \"date\" , fold_stats = False , ): \"\"\"Split the dataset into training and test sets.\"\"\" df_datetime = df [ datetime_column ] if self . train_min_date is not None : train_min_mask = df_datetime >= self . train_min_date else : train_min_mask = df_datetime . notnull () date_range = self . date_range [ ( self . date_range >= df_datetime . min ()) & ( self . date_range <= df_datetime . max ()) ] for start , end in pairwise ( date_range ): fold_info = { \"Start date\" : start , \"End date\" : end } train_mask = train_min_mask & ( df_datetime < start ) train_idx = df . index [ train_mask ] if fold_stats : fold_info [ \"Train\" ] = len ( train_idx ) test_mask = ( df_datetime >= start ) & ( df_datetime < end ) test_idx = df . index [ test_mask ] if self . filter_cold_users : new = np . setdiff1d ( df . loc [ test_idx , user_column ] . unique (), df . loc [ train_idx , user_column ] . unique (), ) new_idx = df . index [ test_mask & df [ user_column ] . isin ( new )] test_idx = np . setdiff1d ( test_idx , new_idx ) test_mask = df . index . isin ( test_idx ) if fold_stats : fold_info [ \"New users\" ] = len ( new ) fold_info [ \"New users interactions\" ] = len ( new_idx ) if self . filter_cold_items : new = np . setdiff1d ( df . loc [ test_idx , item_column ] . unique (), df . loc [ train_idx , item_column ] . unique (), ) new_idx = df . index [ test_mask & df [ item_column ] . isin ( new )] test_idx = np . setdiff1d ( test_idx , new_idx ) test_mask = df . index . isin ( test_idx ) if fold_stats : fold_info [ \"New items\" ] = len ( new ) fold_info [ \"New items interactions\" ] = len ( new_idx ) if self . filter_already_seen : user_item = [ user_column , item_column ] train_pairs = df . loc [ train_idx , user_item ] . set_index ( user_item ) . index test_pairs = df . loc [ test_idx , user_item ] . set_index ( user_item ) . index intersection = train_pairs . intersection ( test_pairs ) test_idx = test_idx [ ~ test_pairs . isin ( intersection )] # test_mask = rd.df.index.isin(test_idx) if fold_stats : fold_info [ \"Known interactions\" ] = len ( intersection ) if fold_stats : fold_info [ \"Test\" ] = len ( test_idx ) yield ( train_idx , test_idx , fold_info ) def get_n_splits ( self , df , datetime_column = \"date\" ): \"\"\"Get n splits.\"\"\" df_datetime = df [ datetime_column ] if self . train_min_date is not None : df_datetime = df_datetime [ df_datetime >= self . train_min_date ] date_range = self . date_range [ ( self . date_range >= df_datetime . min ()) & ( self . date_range <= df_datetime . max ()) ] return max ( 0 , len ( date_range ) - 1 )","title":"TimeRangeSplit"},{"location":"api/#src.validation.TimeRangeSplit.__init__","text":"Initialize TimeRangeSplit. Source code in src/validation.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , start_date , end_date = None , freq = \"D\" , periods = None , tz = None , normalize = False , closed = None , train_min_date = None , filter_cold_users = True , filter_cold_items = True , filter_already_seen = True , ): \"\"\"Initialize TimeRangeSplit.\"\"\" self . start_date = start_date if end_date is None and periods is None : raise ValueError ( \"Either 'end_date' or 'periods' must be non-zero, not both at the same time.\" , ) self . end_date = end_date self . freq = freq self . periods = periods self . tz = tz self . normalize = normalize self . closed = closed self . train_min_date = pd . to_datetime ( train_min_date , errors = \"raise\" ) self . filter_cold_users = filter_cold_users self . filter_cold_items = filter_cold_items self . filter_already_seen = filter_already_seen self . date_range = pd . date_range ( start = start_date , end = end_date , freq = freq , periods = periods , tz = tz , normalize = normalize , closed = closed , ) self . max_n_splits = max ( 0 , len ( self . date_range ) - 1 ) if self . max_n_splits == 0 : raise ValueError ( \"Provided parametrs set an empty date range.\" )","title":"__init__()"},{"location":"api/#src.validation.TimeRangeSplit.get_n_splits","text":"Get n splits. Source code in src/validation.py 137 138 139 140 141 142 143 144 145 146 147 148 def get_n_splits ( self , df , datetime_column = \"date\" ): \"\"\"Get n splits.\"\"\" df_datetime = df [ datetime_column ] if self . train_min_date is not None : df_datetime = df_datetime [ df_datetime >= self . train_min_date ] date_range = self . date_range [ ( self . date_range >= df_datetime . min ()) & ( self . date_range <= df_datetime . max ()) ] return max ( 0 , len ( date_range ) - 1 )","title":"get_n_splits()"},{"location":"api/#src.validation.TimeRangeSplit.split","text":"Split the dataset into training and test sets. Source code in src/validation.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def split ( self , df , user_column = \"user_id\" , item_column = \"item_id\" , datetime_column = \"date\" , fold_stats = False , ): \"\"\"Split the dataset into training and test sets.\"\"\" df_datetime = df [ datetime_column ] if self . train_min_date is not None : train_min_mask = df_datetime >= self . train_min_date else : train_min_mask = df_datetime . notnull () date_range = self . date_range [ ( self . date_range >= df_datetime . min ()) & ( self . date_range <= df_datetime . max ()) ] for start , end in pairwise ( date_range ): fold_info = { \"Start date\" : start , \"End date\" : end } train_mask = train_min_mask & ( df_datetime < start ) train_idx = df . index [ train_mask ] if fold_stats : fold_info [ \"Train\" ] = len ( train_idx ) test_mask = ( df_datetime >= start ) & ( df_datetime < end ) test_idx = df . index [ test_mask ] if self . filter_cold_users : new = np . setdiff1d ( df . loc [ test_idx , user_column ] . unique (), df . loc [ train_idx , user_column ] . unique (), ) new_idx = df . index [ test_mask & df [ user_column ] . isin ( new )] test_idx = np . setdiff1d ( test_idx , new_idx ) test_mask = df . index . isin ( test_idx ) if fold_stats : fold_info [ \"New users\" ] = len ( new ) fold_info [ \"New users interactions\" ] = len ( new_idx ) if self . filter_cold_items : new = np . setdiff1d ( df . loc [ test_idx , item_column ] . unique (), df . loc [ train_idx , item_column ] . unique (), ) new_idx = df . index [ test_mask & df [ item_column ] . isin ( new )] test_idx = np . setdiff1d ( test_idx , new_idx ) test_mask = df . index . isin ( test_idx ) if fold_stats : fold_info [ \"New items\" ] = len ( new ) fold_info [ \"New items interactions\" ] = len ( new_idx ) if self . filter_already_seen : user_item = [ user_column , item_column ] train_pairs = df . loc [ train_idx , user_item ] . set_index ( user_item ) . index test_pairs = df . loc [ test_idx , user_item ] . set_index ( user_item ) . index intersection = train_pairs . intersection ( test_pairs ) test_idx = test_idx [ ~ test_pairs . isin ( intersection )] # test_mask = rd.df.index.isin(test_idx) if fold_stats : fold_info [ \"Known interactions\" ] = len ( intersection ) if fold_stats : fold_info [ \"Test\" ] = len ( test_idx ) yield ( train_idx , test_idx , fold_info )","title":"split()"},{"location":"api/#src.validation.validation_als","text":"Perform k-fold cross-validation on the ALS model.","title":"validation_als()"},{"location":"api/#src.validation.validation_als--parameters","text":"list A list of (train_idx, test_idx, stats) tuples, where train_idx and test_idx are the indices of the training and test sets, respectively, and stats is a dictionary containing statistics about the fold. pandas.DataFrame The interaction matrix, with columns \"user_id\", \"item_id\", and \"weight\".","title":"Parameters"},{"location":"api/#src.validation.validation_als--returns","text":"None Source code in src/validation.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def validation_als ( folds_with_stats , df , users_mapping , items_mapping ): \"\"\" Perform k-fold cross-validation on the ALS model. Parameters ---------- folds_with_stats : list A list of (train_idx, test_idx, stats) tuples, where train_idx and test_idx are the indices of the training and test sets, respectively, and stats is a dictionary containing statistics about the fold. df : pandas.DataFrame The interaction matrix, with columns \"user_id\", \"item_id\", and \"weight\". Returns ------- None \"\"\" wandb . init ( project = \"MFDP\" , name = \"ALS\" ) run_no = 0 for train_idx , test_idx , _ in folds_with_stats : run_no += 1 train = df . loc [ train_idx ] test = df . loc [ test_idx ] train_mat = get_coo_matrix ( train , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () test_mat = get_coo_matrix ( test , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = AlternatingLeastSquares ( factors = 32 , iterations = 30 ) model . fit ( train_mat . T , show_progress = False ) metrics = ranking_metrics_at_k ( model , train_mat . T , test_mat . T , K = 10 , show_progress = False , ) metrics [ \"fold\" ] = run_no wandb . log ( metrics ) data_mat = get_coo_matrix ( df , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = AlternatingLeastSquares ( factors = 32 , iterations = 30 ) model . fit ( data_mat . T , show_progress = False ) with open ( os . path . join ( sys . path [ 0 ], \"./model/als.pickle\" ), \"wb\" ) as f : pickle . dump ( model , f ) artifact = wandb . Artifact ( \"model\" , type = \"model\" ) with artifact . new_file ( os . path . join ( sys . path [ 0 ], \"./model/als.pickle\" ), mode = \"wb\" ) as file : pickle . dump ( model , file ) wandb . log_artifact ( artifact ) wandb . finish ()","title":"Returns"},{"location":"api/#src.validation.validation_bm25","text":"Validate TF-IDF recommender using k-fold cross-validation and log results to WandB.","title":"validation_bm25()"},{"location":"api/#src.validation.validation_bm25--parameters","text":"list List of tuples containing train and test indices and fold statistics. pd.DataFrame Input dataframe containing user-item interaction data. dict Dictionary mapping user IDs to integer indices. dict Dictionary mapping item IDs to integer indices.","title":"Parameters"},{"location":"api/#src.validation.validation_bm25--returns","text":"None Source code in src/validation.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 def validation_bm25 ( folds_with_stats , df , users_mapping , items_mapping ): \"\"\" Validate TF-IDF recommender using k-fold cross-validation and log results to WandB. Parameters ---------- folds_with_stats : list List of tuples containing train and test indices and fold statistics. df : pd.DataFrame Input dataframe containing user-item interaction data. users_mapping : dict Dictionary mapping user IDs to integer indices. items_mapping : dict Dictionary mapping item IDs to integer indices. Returns ------- None \"\"\" wandb . init ( project = \"MFDP\" , name = \"BM25\" ) run_no = 0 for train_idx , test_idx , _ in folds_with_stats : run_no += 1 train = df . loc [ train_idx ] test = df . loc [ test_idx ] train_mat = get_coo_matrix ( train , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () test_mat = get_coo_matrix ( test , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = BM25Recommender ( K = 10 ) model . fit ( train_mat , show_progress = False ) metrics = ranking_metrics_at_k ( model , train_mat , test_mat , K = 10 , show_progress = False , ) metrics [ \"fold\" ] = run_no wandb . log ( metrics ) data_mat = get_coo_matrix ( df , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = BM25Recommender ( K = 10 ) model . fit ( data_mat , show_progress = False ) with open ( os . path . join ( sys . path [ 0 ], \"./model/bm25.pickle\" ), \"wb\" ) as f : pickle . dump ( model , f ) artifact = wandb . Artifact ( \"model\" , type = \"model\" ) with artifact . new_file ( os . path . join ( sys . path [ 0 ], \"./model/bm25.pickle\" ), mode = \"wb\" ) as file : pickle . dump ( model , file ) wandb . log_artifact ( artifact ) wandb . finish ()","title":"Returns"},{"location":"api/#src.validation.validation_bpr","text":"Run k-fold cross-validation for Bayesian Personalized Ranking (BPR). For each fold, fit a BPR model on the training set, and evaluate it on the test set using ranking metrics@k. The metrics are printed to the console.","title":"validation_bpr()"},{"location":"api/#src.validation.validation_bpr--parameters","text":"list of tuple of 3 arrays A list of folds, where each fold is represented as a tuple of 3 arrays: the training indices, test indices, and a dictionary of fold statistics. pandas.DataFrame The input DataFrame containing the user-item interactions.","title":"Parameters"},{"location":"api/#src.validation.validation_bpr--returns","text":"None Source code in src/validation.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def validation_bpr ( folds_with_stats , df , users_mapping , items_mapping ): \"\"\"Run k-fold cross-validation for Bayesian Personalized Ranking (BPR). For each fold, fit a BPR model on the training set, and evaluate it on the test set using ranking metrics@k. The metrics are printed to the console. Parameters ---------- folds_with_stats : list of tuple of 3 arrays A list of folds, where each fold is represented as a tuple of 3 arrays: the training indices, test indices, and a dictionary of fold statistics. df : pandas.DataFrame The input DataFrame containing the user-item interactions. Returns ------- None \"\"\" wandb . init ( project = \"MFDP\" , name = \"BPR\" ) run_no = 0 for train_idx , test_idx , _ in folds_with_stats : run_no += 1 train = df . loc [ train_idx ] test = df . loc [ test_idx ] train_mat = get_coo_matrix ( train , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () test_mat = get_coo_matrix ( test , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = BayesianPersonalizedRanking ( factors = 32 , iterations = 30 ) model . fit ( train_mat . T , show_progress = False ) metrics = ranking_metrics_at_k ( model , train_mat . T , test_mat . T , K = 10 , show_progress = False , ) metrics [ \"fold\" ] = run_no wandb . log ( metrics ) data_mat = get_coo_matrix ( df , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = BayesianPersonalizedRanking ( factors = 32 , iterations = 30 ) model . fit ( data_mat . T , show_progress = False ) with open ( os . path . join ( sys . path [ 0 ], \"./model/bpr.pickle\" ), \"wb\" ) as f : pickle . dump ( model , f ) artifact = wandb . Artifact ( \"model\" , type = \"model\" ) with artifact . new_file ( os . path . join ( sys . path [ 0 ], \"./model/bpr.pickle\" ), mode = \"wb\" ) as file : pickle . dump ( model , file ) wandb . log_artifact ( artifact ) wandb . finish ()","title":"Returns"},{"location":"api/#src.validation.validation_tfidf","text":"Train and evaluate a TF-IDF recommender on cross-validation folds.","title":"validation_tfidf()"},{"location":"api/#src.validation.validation_tfidf--parameters","text":"List[Tuple[np.ndarray, np.ndarray, Dict[str, Any]]] List of tuples where each tuple contains train indices, test indices, and some stats about the fold. pd.DataFrame The ratings dataframe. Dict Mapping from user ids to integers. Dict Mapping from item ids to integers.","title":"Parameters"},{"location":"api/#src.validation.validation_tfidf--returns","text":"None Source code in src/validation.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def validation_tfidf ( folds_with_stats , df , users_mapping , items_mapping ): \"\"\" Train and evaluate a TF-IDF recommender on cross-validation folds. Parameters ---------- folds_with_stats : List[Tuple[np.ndarray, np.ndarray, Dict[str, Any]]] List of tuples where each tuple contains train indices, test indices, and some stats about the fold. df : pd.DataFrame The ratings dataframe. users_mapping : Dict Mapping from user ids to integers. items_mapping : Dict Mapping from item ids to integers. Returns ------- None \"\"\" wandb . init ( project = \"MFDP\" , name = \"TF-IDF\" ) run_no = 0 for train_idx , test_idx , _ in folds_with_stats : run_no += 1 train = df . loc [ train_idx ] test = df . loc [ test_idx ] train_mat = get_coo_matrix ( train , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () test_mat = get_coo_matrix ( test , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = TFIDFRecommender ( K = 10 ) model . fit ( train_mat , show_progress = False ) metrics = ranking_metrics_at_k ( model , train_mat , test_mat , K = 10 , show_progress = False , ) metrics [ \"fold\" ] = run_no wandb . log ( metrics ) data_mat = get_coo_matrix ( df , users_mapping = users_mapping , items_mapping = items_mapping , ) . tocsr () model = TFIDFRecommender ( K = 10 ) model . fit ( data_mat , show_progress = False ) with open ( os . path . join ( sys . path [ 0 ], \"./model/tfidf.pickle\" ), \"wb\" ) as f : pickle . dump ( model , f ) artifact = wandb . Artifact ( \"model\" , type = \"model\" ) with artifact . new_file ( os . path . join ( sys . path [ 0 ], \"./model/tfidf.pickle\" ), mode = \"wb\" ) as file : pickle . dump ( model , file ) wandb . log_artifact ( artifact ) wandb . finish ()","title":"Returns"},{"location":"api/#app_1","text":"","title":"App"},{"location":"api/#app.create_columns_with_data","text":"Creates k columns with data in Streamlit.","title":"create_columns_with_data()"},{"location":"api/#app.create_columns_with_data--parameters","text":"streamlit The Streamlit library. int The number of columns to create. list The list of dictionaries containing the data. Source code in app.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def create_columns_with_data ( st , k : int , data : list ): \"\"\" Creates k columns with data in Streamlit. Parameters ---------- st : streamlit The Streamlit library. k : int The number of columns to create. data : list The list of dictionaries containing the data. \"\"\" if len ( data ) < k : st . error ( \"The number of data points should be greater than or equal to k.\" ) return num_data = len ( data ) num_columns = min ( k , num_data ) columns = st . columns ( num_columns ) for i in range ( num_columns ): item = data . iloc [ i ] # Search for a random photo on Unsplash # photo = api.photo.random() # Get the URL of the photo # photo_url = photo[0].urls.raw # Download the image # response = requests.get(photo_url) # img = Image.open(BytesIO(response.content)) # Resize the image to 400x300 pixels # img = img.resize((200, 300)) # Display the image # columns[i].image(img, width=200) # Display the title columns [ i ] . write ( f '** { item [ \"title\" ] } **' ) # Display the author, year, and genres columns [ i ] . write ( f \"**\u0410\u0432\u0442\u043e\u0440:** { item [ 'authors' ] } \" ) columns [ i ] . write ( f \"**\u0413\u043e\u0434:** { item [ 'year' ] } \" ) columns [ i ] . write ( f \"**\u0416\u0430\u043d\u0440\u044b:** { item [ 'genres' ] } \" )","title":"Parameters"}]}